# -*- coding: utf-8 -*-
"""Lab10_B21CH025

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hzooRxc7AT5lni8jMVMBWz-ridh87cLq
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import time

import warnings
warnings.filterwarnings('ignore')

import torch #python #keras #tensorflow #pytorch
import torch.nn as nn
from torch.autograd import Variable

import pandas as pd

from sklearn.utils import shuffle
from torchsummary import summary

df = pd.read_csv("/content/drive/MyDrive/IML Lab/diabetes (1) (3).csv", header=None)

df.head()

from sklearn.preprocessing import OrdinalEncoder
Ordinal_encoder = OrdinalEncoder()

def jointlyTransform(df_columns):
    return Ordinal_encoder.fit_transform(df_columns)
df.iloc[:,[0]]=jointlyTransform(df.iloc[:,[0]])


column_names = ['sex', 'length', 'diameter', 'height', 'whole weight', 'shucked weight', 'Viscera weight', 'shell weight', 'rings']
df.columns = column_names

df.head()

from sklearn.model_selection  import train_test_split
# y = dataframe['PRICE']
# X = dataframe.iloc[:,0:8]
y = df['rings']
X = df.iloc[:,0:-1]

# Split the data into a training set and a test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)
print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)

l = []
for i in y_train:
  if(i>=1 and i<= 8):
    l.append(0)
  elif(i==9 or i==10):
    l.append(1)
  else:
    l.append(2)
y_train2 = np.array(l)
l =[]
for i in y_test:
  if(i>=1 and i<= 8):
    l.append(0)
  elif(i==9 or i==10):
    l.append(1)
  else:
    l.append(2)
y_test2 = np.array(l)

#Define training hyperprameters.
batch_size = 32 #sample batch
num_epochs = 1000 #number times dataset seen
learning_rate = 0.01
size_hidden_1 = 100 #neurons
size_hidden_2 = 100 #neurons
num_classes = 28

#Calculate some other hyperparameters based on data.  
batch_no = len(X_train) // batch_size  #batches
cols = X_train.shape[1] #Number of columns in input matrix

print(len(X_train))

#Create the model
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
class Net(torch.nn.Module):
    def __init__(self, num_inputs, size_hidden_1, n_output):
        super(Net, self).__init__()
        self.hidden_layer_1 = torch.nn.Linear(num_inputs, size_hidden_1)   # hidden layer
        self.activation_1 = torch.nn.Tanh() # activation layer
        #self.hidden_layer_2 = torch.nn.Linear(num_inputs, size_hidden_2)   # hidden layer
        #self.activation_2 = torch.nn.Tanh() # activation layer
        #self.hidden_layer_3 = torch.nn.Linear(size_hidden_1, size_hidden_2)   # hidden layer
        #self.activation_3 = torch.nn.ReLU() # activation layer
        self.output_layer = torch.nn.Linear(size_hidden_1, n_output)   # output layer
        self.output_act = torch.nn.Sigmoid()

    def forward(self, x):
        x1 = self.activation_1(self.hidden_layer_1(x))     # activation function for hidden layer
        #x2= self.activation_2(self.hidden_layer_2(x))      # activation function for hidden layer
        #x3 = torch.add(x1,x2)
        x = self.output_act(self.output_layer(x1))                    # output
        return x

n_classes2=3
net = Net(cols, size_hidden_1, n_classes2)
#summary(net, (1, 4))

optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate)
loss_func = torch.nn.CrossEntropyLoss()

X_train_copy = X_train

# Commented out IPython magic to ensure Python compatibility.
for epoch in range(num_epochs):
    #Shuffle just mixes up the dataset between epocs
    X_train, y_train2 = shuffle(X_train, y_train2)

    train_acc = 0.0
    running_loss = 0.0

    # Mini batch learning
    for i in range(batch_no):
        start = i * batch_size
        end = start + batch_size
        inputs = Variable(torch.FloatTensor(X_train[start:end]))
        labels = Variable(torch.LongTensor(y_train2[start:end]))
        
        # zero the parameter gradients
        optimizer.zero_grad()

        # forward + backward + optimize
        outputs = net(inputs)

        # print(labels)
        # loss = criterion(outputs, torch.unsqueeze(labels, dim=1))
        loss = loss_func(outputs, labels)
        
        loss.backward()
        optimizer.step()

        # print statistics
        running_loss += loss.item()
        acc = get_accuracy(outputs, labels, batch_size)
        train_acc += acc
      
    #print('Epoch: %d | Loss: %.4f | Train Accuracy: %.2f' \
#           %(epoch+1, running_loss / (i+1), train_acc/(i+1)))  
    running_loss = 0.0

def test_accuracy(X_test, y_test, batch_size):
  batch_no_test = len(X_test) // batch_size
  test_acc=0
  ##print(batch_no_test)
  for i in range(batch_no_test):
      start = i * batch_size
      end = start + batch_size
      inputs = Variable(torch.FloatTensor(X_test[start:end]))
      labels = Variable(torch.LongTensor(y_test[start:end]))

      # forward + backward + optimize
      outputs = net(inputs)
      acc = get_accuracy(outputs, labels, batch_size)
      test_acc += acc

  print(test_acc/(i+1))
test_accuracy(X_test, y_test2, batch_size)